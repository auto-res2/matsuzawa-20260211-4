# @package _global_
# PR-AutoCoT (Paraphrase-Robust Auto-CoT) - Proposed Method
# Inference-only chain-of-thought demonstration construction with paraphrase robustness filtering

run_id: proposed-gsm8k

method:
  name: pr-autocot
  type: proposed
  description: "Paraphrase-Robust Auto-CoT with semantic robustness scoring"
  
  # Diverse candidate selection (Auto-CoT standard)
  clustering:
    method: kmeans
    vectorizer: tfidf  # or sbert
    k: 6  # number of representative questions/demos (reduced from 8 for faster execution)
  
  # Self-consistency scoring
  self_consistency:
    num_samples: 3  # m stochastic solutions (reduced from 5 for faster execution)
    temperature: 0.7
  
  # Paraphrase-consistency scoring (novel component)
  paraphrase_consistency:
    num_paraphrases: 2  # P meaning-preserving paraphrases (reduced from 3 for faster execution)
    temperature: 0.3  # low temp for paraphrase solving
  
  # Reliability weighting
  reliability:
    combining: sqrt_product  # sqrt(s_i * p_i), or "min" for ablation
    threshold: 0.5  # τ: keep demos with w_i >= τ
  
  # Final inference
  inference:
    strategy: greedy  # greedy decoding for test questions
    temperature: 0.0

model:
  name: gpt-3.5-turbo  # Base LLM for all steps
  provider: openai
  max_tokens: 512
  cache_dir: .cache

dataset:
  name: gsm8k
  split_train: train[:300]  # Subsample for demo pool (unlabeled use) - reduced for faster execution
  split_test: test[:100]  # Subsample for evaluation - reduced for faster execution
  cache_dir: .cache

inference:
  task_type: math_word_problem
  extract_answer: true  # Extract final numeric answer
  sanity_check_samples: 10  # Number of samples in sanity_check mode
